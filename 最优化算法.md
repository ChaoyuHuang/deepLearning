## 最优化算法

### SGD(随机梯度下降)
通常指的是mini-batch gradient descent
SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：
gt为上一轮损失函数关于参数θ的梯度
更新∆θ = -学习率*gt

#### 缺点：
<font face="微软雅黑" size=3 color=#FF0000 >选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了</font>

<font face="微软雅黑" size=3 color=#FF0000 >SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点【原来写的是“容易困于鞍点”，经查阅论文发现，其实在合适的初始化和step size的情况下，鞍点的影响并没这么大</font>

## Momentum
momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度
mt = u * m(t-1) + gt
∆θ  = -学习率 * mt

>1.下降初期时，使用上一次参数更新，下降方向一致，乘上较大的u能够进行很好的加速

> 2.下降中后期时，在局部最小值来回震荡的时候梯度趋向于0，u使得更新幅度增大，跳出陷阱

>3.在梯度改变方向的时候，u能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛

### Adam优点：
<mark yellow> Adam使用动量和自适应学习率来加快收敛速度。
计算效率高
> 1 很少的内存需求

> 2 梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）

> 3 非常适合数据和/或参数较大的问题

>4 适用于非固定目标

>5 适用于非常嘈杂和/或稀疏梯度的问题

>6 超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）




### 经验之谈
1、对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值

2、SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠

3、如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。

4、Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。

5、在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果